% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Xing.R
\name{XingMethod}
\alias{XingMethod}
\title{This function computes Xing's global distance metric learning classification
algorithm.}
\usage{
XingMethod(Y, X, S = NULL, D = NULL,
                   learning_rate = 0.1, epsilon = 0.01,
                   error = 1e-10, max_iterations = 1000)
}
\arguments{
\item{Y}{vector of non-negative integer labels corresponding to each data point.}

\item{X}{Input numeric matrix where each row is a data point whose
label is the corresponding entry in \code{Y} and each column is a
variable.}

\item{S}{A \code{n * 2} similarity matrix describing the constraints of data points with
with the same class label.
Each row of the matrix is a pair of indices of two data points in \code{X}
which belong to the same class. For example, pair(1, 3)
says that the first data point is in the same class as the third
data point. Default value is \code{S = NULL} in which case
\code{S} is computed in full. Use this parameter to define a smaller
similarity matrix which is appropriate to your given problem e.g via sampling
methods. The indices in \code{S} should range between 1 and \code{nrow(X)}.}

\item{D}{A \code{n * 2} disimilarity matrix describing the constraints of data points with
with a different class label.
Each row of the matrix is a pair of indices of two data points in \code{X}
which belong to different classes. For example, pair(1, 3)
says that the first data point is in a different class than the third
data point. Default value is \code{D = NULL} in which case
\code{D} is computed in full. Use this parameter to define a smaller disimilarity
matrix which is appropriate to your given problem e.g via sampling
methods. The indices in \code{D} should range between 1 and \code{nrow(X)}.}

\item{learning_rate}{The learning rate to be used in the solver. Default value is
is \code{learning_rate = 0.1}.}

\item{epsilon}{Threshold for convergence of the gradient method. Default value is
\code{epsilon = 0.01}.}

\item{error}{Threshold to be used when projecting onto the constraint
set. Default value is \code{error = 1e-10}.}

\item{max_iterations}{The maximum number of iterations to be processed
in the solver. Default value is \code{max_iterations = 1000}.}
}
\value{
This function returns a list with the following items:
\item{XingTransform}{The matrix under which the data was transformed. The
              multiplication of this matrix with its transpose gives
              the matrix used in the Mahalanobis
              metric.}
\item{TransformedX}{The transformed original data \code{X} which was transformed
                    using the Xing Transform i.e \eqn{TransformedX = X *
                    XingTransform}}.
}
\description{
This function computes Xing's global distance metric learning
             classification algorithm as described in [1]. See the Vignette
             by using the command \code{browseVignette("DistanceLearning")}
             for an introduction to using Xing's global
             distance metric learning method.
}
\details{
See the Vignette by using the command
         \code{browseVignette("DistanceLearning")}
         for an introduction to using Xing's method.
}
\examples{
# Load data from package DistanceLearning
library(DistanceLearning)
library(class)
fname <- system.file("extdata", "example_data.csv", package="DistanceLearning")
df <- read.csv(fname)
Y <- as.integer(df$y)
X <- as.matrix(df[,c(2,3)])
sample_points <- sample(1:nrow(X), 180, replace = FALSE)
subX <- X[sample_points,]
subY <- Y[sample_points]

# Learn the metric, and get the transformed data
result <- XingMethod(subY, subX)
XingMetric <- result$XingTransform
transformedX <- result$transformedX

# Get the accuracy of KNN classification without applying the new metric
yhat <- knn(subX, X[-sample_points,], subY, k = 5)
Accuracy <- length(which(Y[-sample_points] == yhat))/length(Y[-sample_points])
Accuracy

# Get the accuracy of KNN classification after applying the new metric
transformednewX <- X[-sample_points,] \%*\% XingMetric
yhat2 <- knn(transformedX, transformednewX, subY, k = 5)
Accuracy2 <- length(which(Y[-sample_points] == yhat2))/length(Y[-sample_points])
Accuracy2

}
\references{
[1] Eric P. Xing, Michael I. Jordan, Stuart J Russell, and
                Andrew Y. Ng. Distance Metric Learning with Application
                to Clustering with Side-Information. In S. Becker,
                S. Thrun, and K. Obermayer, editors, Advances in Neural
                Information Processing Systems 15, pages 521-528.
                MIT Press, 2003.
}
\author{
Carl Tony Fakhry, Ping Chen, Rahul Kulkarni and Kourosh Zarringhalam
}
